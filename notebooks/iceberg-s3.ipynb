{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd3fb5-697b-41f7-90f2-d8e51e555f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iceberg.hive import HiveTables\n",
    "\n",
    "# instantiate Hive Tables\n",
    "#conf = {\"hive.metastore.uris\": 'thrift://{hms_host}:{hms_port}'}\n",
    "conf = {\"hive.metastore.warehouse.dir\": \"file://home/jovyan/notebooks/iceberg/warehouse\" }\n",
    "\n",
    "tables = HiveTables(conf)\n",
    "\n",
    "# load table\n",
    "tbl = tables.load(\"db.table\")\n",
    "\n",
    "# inspect metadata\n",
    "print(tbl.schema())\n",
    "print(tbl.spec())\n",
    "print(tbl.location())\n",
    "\n",
    "# get table level record count\n",
    "from pprint import pprint\n",
    "pprint(int(tbl.current_snapshot().summary.get(\"total-records\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fbf47-ba5a-443a-b4ae-c28567f8207d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab703b9-b325-4c3a-bc98-059a9b1affb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ad4dd-3c87-41c8-aa61-a5d95832cd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8537d11-dfdf-4865-b7dd-b17ca7192922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f13616e-9902-4265-b4fd-162ef08b033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple, Union, Dict, Any\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from enum import Enum\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "from feast import Entity, Feature, FeatureView, ValueType\n",
    "from yummy import ParquetDataSource, CsvDataSource, DeltaDataSource\n",
    "\n",
    "\n",
    "class DataType(str, Enum):\n",
    "    csv = \"csv\"\n",
    "    parquet = \"parquet\"\n",
    "    delta = \"delta\"\n",
    "\n",
    "class Generator(ABC):\n",
    "\n",
    "    @staticmethod\n",
    "    def entity() -> Entity:\n",
    "        return Entity(name=\"entity_id\", value_type=ValueType.INT64, description=\"entity id\",)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_entities(size: int):\n",
    "        return np.random.choice(size, size=size, replace=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def entity_df(size:int = 10):\n",
    "        entities=Generator.generate_entities(size)\n",
    "        entity_df = pd.DataFrame(data=entities, columns=['entity_id'])\n",
    "        entity_df[\"event_timestamp\"]=datetime(2021, 10, 1, 23, 59, 42, tzinfo=timezone.utc)\n",
    "        return entity_df\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data(entities, year=2021, month=10, day=1) -> pd.DataFrame:\n",
    "        n_samples=len(entities)\n",
    "        X, y = make_hastie_10_2(n_samples=n_samples, random_state=0)\n",
    "        df = pd.DataFrame(X, columns=[\"f0\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\"])\n",
    "        df[\"y\"]=y\n",
    "        df['entity_id'] = entities\n",
    "        df['datetime'] = pd.to_datetime(\n",
    "                np.random.randint(\n",
    "                    datetime(year, month, day, 0,tzinfo=timezone.utc).timestamp(),\n",
    "                    datetime(year, month, day, 22,tzinfo=timezone.utc).timestamp(),\n",
    "                    size=n_samples),\n",
    "            unit=\"s\", #utc=True\n",
    "        )\n",
    "        df['created'] = pd.to_datetime(\n",
    "                datetime.now(), #utc=True\n",
    "                )\n",
    "        df['month_year'] = pd.to_datetime(datetime(year, month, day, 0, tzinfo=timezone.utc), utc=True)\n",
    "        return df\n",
    "\n",
    "    @property\n",
    "    def data_type(self) -> DataType:\n",
    "        raise NotImplementedError(\"Data type not defined\")\n",
    "\n",
    "    @property\n",
    "    def features(self):\n",
    "        return [\n",
    "            Feature(name=\"f0\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f1\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f2\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f3\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f4\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f5\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f6\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f7\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f8\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"f9\", dtype=ValueType.FLOAT),\n",
    "            Feature(name=\"y\", dtype=ValueType.FLOAT),\n",
    "        ]\n",
    "\n",
    "    def generate(self, path: str, size: int = 10, year: int = 2021, month: int = 10, day: int = 1) -> Tuple[FeatureView, str]:\n",
    "        entities = Generator.generate_entities(size)\n",
    "        df = Generator.generate_data(entities, year, month, day)\n",
    "        self.write_data(df, path)\n",
    "        return self.prepare_features(path)\n",
    "\n",
    "    @abstractmethod\n",
    "    def write_data(self, df: pd.DataFrame, path: str):\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_source(self, path: str):\n",
    "        ...\n",
    "\n",
    "    def prepare_features(self, path: str) -> Tuple[FeatureView, str]:\n",
    "        source = self.prepare_source(path)\n",
    "        name = f\"fv_{self.data_type}\"\n",
    "        return FeatureView(\n",
    "            name=name,\n",
    "            entities=[\"entity_id\"],\n",
    "            ttl=Duration(seconds=3600*24*20),\n",
    "            features=self.features,\n",
    "            online=True,\n",
    "            input=source,\n",
    "            tags={},), name\n",
    "\n",
    "\n",
    "class CsvGenerator(Generator):\n",
    "\n",
    "    @property\n",
    "    def data_type(self) -> DataType:\n",
    "        return DataType.csv\n",
    "\n",
    "    def write_data(self, df: pd.DataFrame, path: str):\n",
    "        df.to_csv(path)\n",
    "\n",
    "    def prepare_source(self, path: str):\n",
    "        return CsvDataSource(\n",
    "            path=path,\n",
    "            event_timestamp_column=\"datetime\",\n",
    "        )\n",
    "\n",
    "class ParquetGenerator(Generator):\n",
    "\n",
    "    @property\n",
    "    def data_type(self) -> DataType:\n",
    "        return DataType.parquet\n",
    "\n",
    "    def write_data(self, df: pd.DataFrame, path: str):\n",
    "        df.to_parquet(path)\n",
    "\n",
    "    def prepare_source(self, path: str):\n",
    "        return ParquetDataSource(\n",
    "            path=path,\n",
    "            event_timestamp_column=\"datetime\",\n",
    "        )\n",
    "\n",
    "class DeltaGenerator(Generator):\n",
    "\n",
    "    @property\n",
    "    def data_type(self) -> DataType:\n",
    "        return DataType.delta\n",
    "\n",
    "    def write_data(self, df: pd.DataFrame, path: str):\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark import SparkConf\n",
    "\n",
    "        spark = SparkSession.builder.config(conf=SparkConf().setAll(\n",
    "            [\n",
    "                (\"spark.master\", \"local[*]\"),\n",
    "                (\"spark.ui.enabled\", \"false\"),\n",
    "                (\"spark.eventLog.enabled\", \"false\"),\n",
    "                (\"spark.sql.session.timeZone\", \"UTC\"),\n",
    "                (\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\"),\n",
    "                (\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            ]\n",
    "        )).getOrCreate()\n",
    "\n",
    "        spark.createDataFrame(df).write.format(\"delta\").mode(\"append\").save(path)\n",
    "\n",
    "    def prepare_source(self, path: str):\n",
    "        return DeltaDataSource(\n",
    "            path=path,\n",
    "            event_timestamp_column=\"datetime\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9ff21a-81e4-4be5-869a-dd4632b9b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IcebergGenerator(Generator):\n",
    "\n",
    "    @property\n",
    "    def data_type(self) -> DataType:\n",
    "        return DataType.delta\n",
    "\n",
    "    def write_data(self, df: pd.DataFrame, path: str):\n",
    "        import os\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark import SparkConf\n",
    "\n",
    "        dir_name=os.path.dirname(path)\n",
    "        db_name=os.path.basename(path)\n",
    "        \n",
    "        spark = SparkSession.builder.config(conf=SparkConf().setAll(\n",
    "            [\n",
    "                (\"spark.master\", \"local[*]\"),\n",
    "                (\"spark.ui.enabled\", \"false\"),\n",
    "                (\"spark.eventLog.enabled\", \"false\"),\n",
    "                (\"spark.sql.session.timeZone\", \"UTC\"),\n",
    "                (\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"),\n",
    "                #(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\"),\n",
    "                #(\"spark.sql.catalog.spark_catalog.type\",\"hive\"),\n",
    "                (\"spark.sql.catalog.local\",\"org.apache.iceberg.spark.SparkCatalog\"),\n",
    "                (\"spark.sql.catalog.local.type\",\"hadoop\"),\n",
    "                #(\"spark.sql.catalog.local.warehouse\",dir_name),\n",
    "                #(\"spark.sql.catalog.local.catalog-impl\",\"org.apache.iceberg.hadoop.HadoopCatalog\"),\n",
    "                #(\"spark.sql.catalog.local.io-impl\",\"org.apache.iceberg.aws.s3.S3FileIO\"),\n",
    "                (\"spark.sql.catalog.local.warehouse\",\"s3a://mybucket3\"),\n",
    "                (\"spark.hadoop.fs.s3a.endpoint\",\"http://minio:9000\"),\n",
    "                (\"spark.hadoop.fs.s3a.access.key\",\"minioadmin\"),\n",
    "                (\"spark.hadoop.fs.s3a.secret.key\",\"minioadmin\"),\n",
    "                (\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"),\n",
    "                (\"spark.hadoop.fs.s3a.path.style.access\",\"true\"),\n",
    "                #(\"fs.s3a.path.style.access\",\"true\"),\n",
    "                #(\"fs.s3a.aws.credentials.provider\",'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider'),\n",
    "                (\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\"),\n",
    "            ]\n",
    "        )).getOrCreate()\n",
    "\n",
    "        spark.sql(f\"CREATE TABLE local.db.table (f0 float, f1 float, f2 float, f3 float, f4 float, f5 float, f6 float, f7 float, f8 float, f9 float, y float, entity_id int, datetime date, created date, month_year date) USING iceberg\")\n",
    "        spark.createDataFrame(df).write.format(\"iceberg\").mode(\"append\").save(db_name)\n",
    "\n",
    "    def prepare_source(self, path: str):\n",
    "        return DeltaDataSource(\n",
    "            path=path,\n",
    "            event_timestamp_column=\"datetime\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5b9e3-b433-489b-ad64-6d0dcec8c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = IcebergGenerator()\n",
    "\n",
    "generator.generate(\"/home/jovyan/notebooks/iceberg/warehouse/local.db.table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64feded9-62ce-43e9-8851-e10212e7dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.config(conf=SparkConf().setAll(\n",
    "    [\n",
    "        (\"spark.master\", \"local[*]\"),\n",
    "        (\"spark.ui.enabled\", \"false\"),\n",
    "        (\"spark.eventLog.enabled\", \"false\"),\n",
    "        (\"spark.sql.session.timeZone\", \"UTC\"),\n",
    "        (\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"),\n",
    "        #(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\"),\n",
    "        #(\"spark.sql.catalog.spark_catalog.type\",\"hive\"),\n",
    "        (\"spark.sql.catalog.local\",\"org.apache.iceberg.spark.SparkCatalog\"),\n",
    "        (\"spark.sql.catalog.local.type\",\"hadoop\"),\n",
    "        (\"spark.sql.catalog.local.warehouse\",\"s3a://mybucket2\"),\n",
    "        (\"spark.hadoop.fs.s3a.endpoint\",\"http://minio:9000\"),\n",
    "        (\"spark.hadoop.fs.s3a.access.key\",\"minioadmin\"),\n",
    "        (\"spark.hadoop.fs.s3a.secret.key\",\"minioadmin\"),\n",
    "        (\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"),\n",
    "        (\"spark.hadoop.fs.s3a.path.style.access\",\"true\"),\n",
    "        (\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\"),\n",
    "    ]\n",
    ")).getOrCreate()\n",
    "\n",
    "#spark.sql(\"select * from local.db.table\").collect()\n",
    "spark.read.format('iceberg').load(\"local.db.table\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaedeed-1db8-4cb7-bd1e-28b917066efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.config(conf=SparkConf().setAll(\n",
    "    [\n",
    "        (\"spark.master\", \"local[*]\"),\n",
    "        (\"spark.ui.enabled\", \"false\"),\n",
    "        (\"spark.eventLog.enabled\", \"false\"),\n",
    "        (\"spark.sql.session.timeZone\", \"UTC\"),\n",
    "        (\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"),\n",
    "        #(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\"),\n",
    "        #(\"spark.sql.catalog.spark_catalog.type\",\"hive\"),\n",
    "        (\"spark.sql.catalog.local\",\"org.apache.iceberg.spark.SparkCatalog\"),\n",
    "        (\"spark.sql.catalog.local.type\",\"hadoop\"),\n",
    "        (\"spark.sql.catalog.local.warehouse\",\"s3a://mybucket\"),\n",
    "        (\"spark.hadoop.fs.s3a.endpoint\",\"http://minio:9000\"),\n",
    "        (\"spark.hadoop.fs.s3a.access.key\",\"minioadmin\"),\n",
    "        (\"spark.hadoop.fs.s3a.secret.key\",\"minioadmin\"),\n",
    "        (\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\"),\n",
    "        (\"spark.hadoop.fs.s3a.path.style.access\",\"true\"),\n",
    "        (\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\"),\n",
    "    ]\n",
    ")).getOrCreate()\n",
    "\n",
    "#spark.sql(\"select * from local.mytable_dbz.debeziumcdc_postgres_public_dbz_test\").collect()\n",
    "spark.read.format('iceberg').load(\"local.mytable_dbz.debeziumcdc_postgres_public_dbz_test\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
